#!/usr/bin/env python3
"""
ä¸å‹•ç”£æŸ»å®šãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æ©Ÿèƒ½:
- ãƒ‡ãƒ¼ã‚¿åé›†ãƒ»å‰å‡¦ç†
- è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã®å­¦ç¿’ãƒ»æ¯”è¼ƒ
- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
- è©•ä¾¡ãƒ»å¯è¦–åŒ–
- ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤

ä½¿ç”¨æ–¹æ³•:
python create_model.py [options]
"""

import argparse
import logging
import sys
import json
import pickle
from pathlib import Path
from datetime import datetime
import warnings

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.svm import SVR
from sklearn.model_selection import (
    train_test_split, cross_val_score, GridSearchCV, 
    RandomizedSearchCV, validation_curve
)
from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error
from sklearn.pipeline import Pipeline
import joblib

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class ModelCreator:
    """
    ä¸å‹•ç”£æŸ»å®šãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self, output_dir='../api', random_state=42):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.random_state = random_state
        
        # çµæœä¿å­˜ç”¨
        self.results = {}
        self.best_model = None
        self.best_score = -np.inf
        self.data = None
        self.feature_columns = [
            'éƒ½é“åºœçœŒ', 'å¸‚åŒºç”ºæ‘', 'åœ°åŒº', 'åœŸåœ°é¢ç©', 'å»ºç‰©é¢ç©', 'ç¯‰å¹´æ•°'
        ]
        
        # å¯è¦–åŒ–è¨­å®š
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")
    
    def load_and_prepare_data(self, data_source='sample', test_size=0.2):
        """
        ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨æº–å‚™
        """
        logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹: {data_source}")
        
        if data_source == 'sample':
            self.data = self._generate_enhanced_sample_data()
        elif data_source == 'mlit':
            self.data = self._load_mlit_data()
        elif data_source == 'csv':
            self.data = self._load_csv_data()
        else:
            raise ValueError(f"Unknown data source: {data_source}")
        
        logger.info(f"èª­ã¿è¾¼ã¿å®Œäº†: {len(self.data)} ä»¶")
        
        # ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
        self.data = self._preprocess_data(self.data)
        logger.info(f"å‰å‡¦ç†å¾Œ: {len(self.data)} ä»¶")
        
        # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
        self.X, self.y = self._prepare_features_target()
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            self.X, self.y, test_size=test_size, random_state=self.random_state
        )
        
        logger.info(f"ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Œäº† - è¨“ç·´: {len(self.X_train)}, ãƒ†ã‚¹ãƒˆ: {len(self.X_test)}")
        
        return self.data
    
    def _generate_enhanced_sample_data(self, n_samples=2000):
        """
        æ‹¡å¼µã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
        """
        np.random.seed(self.random_state)
        
        # åœ°åŸŸãƒ‡ãƒ¼ã‚¿ã®æ‹¡å¼µ
        prefectures = ['æ±äº¬éƒ½', 'ç¥å¥ˆå·çœŒ', 'å¤§é˜ªåºœ', 'æ„›çŸ¥çœŒ', 'åŸ¼ç‰çœŒ', 'åƒè‘‰çœŒ', 'å…µåº«çœŒ', 'ç¦å²¡çœŒ']
        cities = {
            'æ±äº¬éƒ½': ['æ¸‹è°·åŒº', 'æ–°å®¿åŒº', 'æ¸¯åŒº', 'åƒä»£ç”°åŒº', 'ä¸­å¤®åŒº', 'ç›®é»’åŒº', 'ä¸–ç”°è°·åŒº'],
            'ç¥å¥ˆå·çœŒ': ['æ¨ªæµœå¸‚', 'å·å´å¸‚', 'ç›¸æ¨¡åŸå¸‚', 'è—¤æ²¢å¸‚', 'èŒ…ãƒ¶å´å¸‚'],
            'å¤§é˜ªåºœ': ['å¤§é˜ªå¸‚', 'å ºå¸‚', 'æ±å¤§é˜ªå¸‚', 'è±Šä¸­å¸‚', 'å¹ç”°å¸‚'],
            'æ„›çŸ¥çœŒ': ['åå¤å±‹å¸‚', 'è±Šç”°å¸‚', 'å²¡å´å¸‚', 'ä¸€å®®å¸‚'],
            'åŸ¼ç‰çœŒ': ['ã•ã„ãŸã¾å¸‚', 'å·å£å¸‚', 'æ‰€æ²¢å¸‚', 'è¶Šè°·å¸‚'],
            'åƒè‘‰çœŒ': ['åƒè‘‰å¸‚', 'èˆ¹æ©‹å¸‚', 'æŸå¸‚', 'æ¾æˆ¸å¸‚'],
            'å…µåº«çœŒ': ['ç¥æˆ¸å¸‚', 'å§«è·¯å¸‚', 'è¥¿å®®å¸‚', 'å°¼å´å¸‚'],
            'ç¦å²¡çœŒ': ['ç¦å²¡å¸‚', 'åŒ—ä¹å·å¸‚', 'ä¹…ç•™ç±³å¸‚']
        }
        
        districts = ['ä¸­å¤®', 'æ±', 'è¥¿', 'å—', 'åŒ—', 'é§…å‰', 'æ–°ç”º', 'æœ¬ç”º', 'æ „', 'ä¸­å¿ƒéƒ¨']
        
        data = []
        
        for _ in range(n_samples):
            prefecture = np.random.choice(prefectures)
            city = np.random.choice(cities[prefecture])
            district = np.random.choice(districts)
            
            # åœ°åŸŸã«ã‚ˆã‚‹ä¾¡æ ¼ä¿‚æ•°
            region_multiplier = {
                'æ±äº¬éƒ½': np.random.uniform(1.5, 3.0),
                'ç¥å¥ˆå·çœŒ': np.random.uniform(1.2, 2.0),
                'å¤§é˜ªåºœ': np.random.uniform(1.1, 1.8),
                'æ„›çŸ¥çœŒ': np.random.uniform(1.0, 1.5),
                'åŸ¼ç‰çœŒ': np.random.uniform(0.9, 1.4),
                'åƒè‘‰çœŒ': np.random.uniform(0.8, 1.3),
                'å…µåº«çœŒ': np.random.uniform(0.9, 1.4),
                'ç¦å²¡çœŒ': np.random.uniform(0.7, 1.2)
            }[prefecture]
            
            # é¢ç©ã®ç”Ÿæˆï¼ˆå¯¾æ•°æ­£è¦åˆ†å¸ƒï¼‰
            land_area = np.random.lognormal(4.5, 0.6)  # å¹³å‡ç´„100ã¡
            building_area = land_area * np.random.uniform(0.6, 1.2)  # å»ºãºã„ç‡è€ƒæ…®
            
            # ç¯‰å¹´æ•°ï¼ˆæŒ‡æ•°åˆ†å¸ƒã§æ–°ã—ã„ç‰©ä»¶ãŒå¤šããªã‚‹ã‚ˆã†ã«ï¼‰
            building_age = np.random.exponential(15)
            building_age = min(building_age, 50)  # æœ€å¤§50å¹´
            
            # ä¾¡æ ¼è¨ˆç®—ï¼ˆã‚ˆã‚Šç¾å®Ÿçš„ãªè¨ˆç®—å¼ï¼‰
            base_price_per_sqm = 300000  # åŸºæº–ä¾¡æ ¼ï¼ˆã¡ã‚ãŸã‚Šï¼‰
            
            # å„è¦å› ã®å½±éŸ¿
            area_factor = (land_area * 0.7 + building_area * 0.3)
            location_factor = region_multiplier
            age_factor = max(0.3, 1.0 - building_age * 0.015)  # ç¯‰å¹´åŠ£åŒ–
            
            # ãƒ©ãƒ³ãƒ€ãƒ è¦å› ï¼ˆå¸‚å ´å¤‰å‹•ãªã©ï¼‰
            random_factor = np.random.uniform(0.8, 1.2)
            
            price = (base_price_per_sqm * area_factor * location_factor * 
                    age_factor * random_factor)
            
            # å¤–ã‚Œå€¤ã®è¿½åŠ ï¼ˆ5%ã®ç¢ºç‡ï¼‰
            if np.random.random() < 0.05:
                price *= np.random.uniform(1.5, 3.0)
            
            data.append({
                'éƒ½é“åºœçœŒ': prefecture,
                'å¸‚åŒºç”ºæ‘': city,
                'åœ°åŒº': district,
                'åœŸåœ°é¢ç©': max(20, land_area),
                'å»ºç‰©é¢ç©': max(15, building_area),
                'ç¯‰å¹´æ•°': int(building_age),
                'å–å¼•ä¾¡æ ¼': int(price)
            })
        
        return pd.DataFrame(data)
    
    def _load_mlit_data(self):
        """
        MLIT APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        """
        try:
            sys.path.append(str(Path(__file__).parent.parent / 'api'))
            from models.data_fetcher import MLITDataFetcher
            
            fetcher = MLITDataFetcher()
            df = fetcher.fetch_trade_data(
                prefecture="æ±äº¬éƒ½",
                from_year=2021,
                to_year=2024
            )
            
            if df.empty:
                logger.warning("MLIT APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                return self._generate_enhanced_sample_data()
            
            return df
            
        except Exception as e:
            logger.error(f"MLIT API ã‚¨ãƒ©ãƒ¼: {e}")
            return self._generate_enhanced_sample_data()
    
    def _load_csv_data(self):
        """
        CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        """
        csv_path = self.output_dir / 'training_data.csv'
        if csv_path.exists():
            return pd.read_csv(csv_path)
        else:
            logger.warning(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}")
            return self._generate_enhanced_sample_data()
    
    def _preprocess_data(self, df):
        """
        ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
        """
        logger.info("ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†é–‹å§‹")
        
        df_clean = df.copy()
        original_count = len(df_clean)
        
        # åˆ—åã®çµ±ä¸€
        column_mapping = {
            'å–å¼•ä¾¡æ ¼ï¼ˆç·é¡ï¼‰': 'å–å¼•ä¾¡æ ¼',
            'é¢ç©ï¼ˆã¡ï¼‰': 'åœŸåœ°é¢ç©'
        }
        df_clean = df_clean.rename(columns=column_mapping)
        
        # å¿…è¦ãªåˆ—ã®ç¢ºèª
        required_columns = ['éƒ½é“åºœçœŒ', 'å¸‚åŒºç”ºæ‘', 'åœ°åŒº', 'åœŸåœ°é¢ç©', 'å»ºç‰©é¢ç©', 'ç¯‰å¹´æ•°', 'å–å¼•ä¾¡æ ¼']
        for col in required_columns:
            if col not in df_clean.columns:
                logger.warning(f"åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {col}")
        
        # æ•°å€¤å‹ã¸ã®å¤‰æ›
        numeric_columns = ['åœŸåœ°é¢ç©', 'å»ºç‰©é¢ç©', 'ç¯‰å¹´æ•°', 'å–å¼•ä¾¡æ ¼']
        for col in numeric_columns:
            if col in df_clean.columns:
                df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
        
        # ç•°å¸¸å€¤ã®é™¤å»
        df_clean = df_clean[
            (df_clean['å–å¼•ä¾¡æ ¼'] > 1_000_000) &
            (df_clean['å–å¼•ä¾¡æ ¼'] < 5_000_000_000) &
            (df_clean['åœŸåœ°é¢ç©'] > 10) &
            (df_clean['åœŸåœ°é¢ç©'] < 5000) &
            (df_clean['å»ºç‰©é¢ç©'] > 10) &
            (df_clean['å»ºç‰©é¢ç©'] < 2000) &
            (df_clean['ç¯‰å¹´æ•°'] >= 0) &
            (df_clean['ç¯‰å¹´æ•°'] <= 100)
        ]
        
        # æ¬ æå€¤ã®é™¤å»
        df_clean = df_clean.dropna()
        
        removed_count = original_count - len(df_clean)
        logger.info(f"å‰å‡¦ç†å®Œäº†: {removed_count} ä»¶å‰Šé™¤ ({removed_count/original_count*100:.1f}%)")
        
        return df_clean
    
    def _prepare_features_target(self):
        """
        ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®æº–å‚™
        """
        X = self.data[self.feature_columns].copy()
        y = self.data['å–å¼•ä¾¡æ ¼']
        
        # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        categorical_columns = ['éƒ½é“åºœçœŒ', 'å¸‚åŒºç”ºæ‘', 'åœ°åŒº']
        self.encoders = {}
        
        for col in categorical_columns:
            if col in X.columns:
                encoder = LabelEncoder()
                X[col] = encoder.fit_transform(X[col].astype(str))
                self.encoders[col] = encoder
        
        return X, y
    
    def create_model_configurations(self):
        """
        ãƒ¢ãƒ‡ãƒ«è¨­å®šã®ä½œæˆ
        """
        configurations = {
            'random_forest': {
                'model': RandomForestRegressor(random_state=self.random_state),
                'params': {
                    'n_estimators': [100, 200, 300],
                    'max_depth': [10, 15, 20, None],
                    'min_samples_split': [2, 5, 10],
                    'min_samples_leaf': [1, 2, 4],
                    'max_features': ['sqrt', 'log2', None]
                }
            },
            
            'gradient_boosting': {
                'model': GradientBoostingRegressor(random_state=self.random_state),
                'params': {
                    'n_estimators': [100, 200, 300],
                    'learning_rate': [0.05, 0.1, 0.15],
                    'max_depth': [3, 5, 7],
                    'min_samples_split': [2, 5, 10],
                    'min_samples_leaf': [1, 2, 4]
                }
            },
            
            'extra_trees': {
                'model': ExtraTreesRegressor(random_state=self.random_state),
                'params': {
                    'n_estimators': [100, 200, 300],
                    'max_depth': [10, 15, 20, None],
                    'min_samples_split': [2, 5, 10],
                    'min_samples_leaf': [1, 2, 4]
                }
            },
            
            'ridge': {
                'model': Pipeline([
                    ('scaler', StandardScaler()),
                    ('regressor', Ridge(random_state=self.random_state))
                ]),
                'params': {
                    'regressor__alpha': [0.1, 1.0, 10.0, 100.0]
                }
            },
            
            'lasso': {
                'model': Pipeline([
                    ('scaler', StandardScaler()),
                    ('regressor', Lasso(random_state=self.random_state))
                ]),
                'params': {
                    'regressor__alpha': [0.1, 1.0, 10.0, 100.0]
                }
            },
            
            'elastic_net': {
                'model': Pipeline([
                    ('scaler', StandardScaler()),
                    ('regressor', ElasticNet(random_state=self.random_state))
                ]),
                'params': {
                    'regressor__alpha': [0.1, 1.0, 10.0],
                    'regressor__l1_ratio': [0.1, 0.5, 0.7, 0.9]
                }
            }
        }
        
        return configurations
    
    def train_and_evaluate_models(self, use_grid_search=True, cv_folds=5):
        """
        ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨è©•ä¾¡
        """
        logger.info("ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ»è©•ä¾¡é–‹å§‹")
        
        configurations = self.create_model_configurations()
        
        for model_name, config in configurations.items():
            logger.info(f"å­¦ç¿’ä¸­: {model_name}")
            
            try:
                if use_grid_search:
                    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
                    grid_search = GridSearchCV(
                        config['model'], 
                        config['params'],
                        cv=cv_folds,
                        scoring='r2',
                        n_jobs=-1,
                        verbose=0
                    )
                    grid_search.fit(self.X_train, self.y_train)
                    best_model = grid_search.best_estimator_
                    best_params = grid_search.best_params_
                else:
                    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å­¦ç¿’
                    best_model = config['model']
                    best_model.fit(self.X_train, self.y_train)
                    best_params = {}
                
                # äºˆæ¸¬
                y_train_pred = best_model.predict(self.X_train)
                y_test_pred = best_model.predict(self.X_test)
                
                # è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—
                metrics = {
                    'train_mae': mean_absolute_error(self.y_train, y_train_pred),
                    'test_mae': mean_absolute_error(self.y_test, y_test_pred),
                    'train_rmse': np.sqrt(mean_squared_error(self.y_train, y_train_pred)),
                    'test_rmse': np.sqrt(mean_squared_error(self.y_test, y_test_pred)),
                    'train_r2': r2_score(self.y_train, y_train_pred),
                    'test_r2': r2_score(self.y_test, y_test_pred),
                    'test_mape': mean_absolute_percentage_error(self.y_test, y_test_pred) * 100
                }
                
                # äº¤å·®æ¤œè¨¼
                cv_scores = cross_val_score(best_model, self.X, self.y, cv=cv_folds, scoring='r2')
                
                # çµæœä¿å­˜
                self.results[model_name] = {
                    'model': best_model,
                    'best_params': best_params,
                    'metrics': metrics,
                    'cv_scores': cv_scores,
                    'cv_mean': cv_scores.mean(),
                    'cv_std': cv_scores.std(),
                    'predictions': {
                        'train': y_train_pred,
                        'test': y_test_pred
                    }
                }
                
                # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®æ›´æ–°
                if metrics['test_r2'] > self.best_score:
                    self.best_score = metrics['test_r2']
                    self.best_model = best_model
                    self.best_model_name = model_name
                
                logger.info(f"{model_name} å®Œäº† - Test RÂ²: {metrics['test_r2']:.3f}")
                
            except Exception as e:
                logger.error(f"{model_name} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        logger.info(f"æœ€å„ªç§€ãƒ¢ãƒ‡ãƒ«: {self.best_model_name} (RÂ² = {self.best_score:.3f})")
    
    def create_visualizations(self):
        """
        å¯è¦–åŒ–ã®ä½œæˆ
        """
        logger.info("å¯è¦–åŒ–ä½œæˆä¸­")
        
        # çµæœæ¯”è¼ƒç”¨ã®DataFrameä½œæˆ
        comparison_data = []
        for model_name, result in self.results.items():
            metrics = result['metrics']
            comparison_data.append({
                'Model': model_name,
                'Test RÂ²': metrics['test_r2'],
                'Test MAE': metrics['test_mae'],
                'Test RMSE': metrics['test_rmse'],
                'Test MAPE': metrics['test_mape'],
                'CV Mean': result['cv_mean'],
                'CV Std': result['cv_std']
            })
        
        comparison_df = pd.DataFrame(comparison_data)
        
        # 1. ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã‚°ãƒ©ãƒ•
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # RÂ²ã‚¹ã‚³ã‚¢æ¯”è¼ƒ
        axes[0, 0].bar(comparison_df['Model'], comparison_df['Test RÂ²'])
        axes[0, 0].set_title('Test RÂ² Score Comparison')
        axes[0, 0].set_ylabel('RÂ² Score')
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # MAEæ¯”è¼ƒ
        axes[0, 1].bar(comparison_df['Model'], comparison_df['Test MAE'])
        axes[0, 1].set_title('Test MAE Comparison')
        axes[0, 1].set_ylabel('MAE (å††)')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # äº¤å·®æ¤œè¨¼ã‚¹ã‚³ã‚¢
        axes[1, 0].errorbar(comparison_df['Model'], comparison_df['CV Mean'], 
                           yerr=comparison_df['CV Std'], fmt='o', capsize=5)
        axes[1, 0].set_title('Cross-Validation RÂ² Score')
        axes[1, 0].set_ylabel('CV RÂ² Score')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # MAPEæ¯”è¼ƒ
        axes[1, 1].bar(comparison_df['Model'], comparison_df['Test MAPE'])
        axes[1, 1].set_title('Test MAPE Comparison')
        axes[1, 1].set_ylabel('MAPE (%)')
        axes[1, 1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'model_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç²¾åº¦
        best_result = self.results[self.best_model_name]
        y_test_pred = best_result['predictions']['test']
        
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        # å®Ÿæ¸¬å€¤ vs äºˆæ¸¬å€¤
        axes[0].scatter(self.y_test, y_test_pred, alpha=0.6)
        axes[0].plot([self.y_test.min(), self.y_test.max()], 
                    [self.y_test.min(), self.y_test.max()], 'r--', lw=2)
        axes[0].set_xlabel('å®Ÿæ¸¬å€¤')
        axes[0].set_ylabel('äºˆæ¸¬å€¤')
        axes[0].set_title(f'{self.best_model_name} - å®Ÿæ¸¬å€¤ vs äºˆæ¸¬å€¤')
        
        # æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ
        residuals = self.y_test - y_test_pred
        axes[1].scatter(y_test_pred, residuals, alpha=0.6)
        axes[1].axhline(y=0, color='r', linestyle='--')
        axes[1].set_xlabel('äºˆæ¸¬å€¤')
        axes[1].set_ylabel('æ®‹å·®')
        axes[1].set_title(f'{self.best_model_name} - æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'best_model_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆè©²å½“ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®å ´åˆï¼‰
        if hasattr(self.best_model, 'feature_importances_'):
            feature_importance = self.best_model.feature_importances_
            feature_names = self.feature_columns
            
            plt.figure(figsize=(10, 6))
            indices = np.argsort(feature_importance)[::-1]
            plt.bar(range(len(feature_importance)), feature_importance[indices])
            plt.xticks(range(len(feature_importance)), 
                      [feature_names[i] for i in indices], rotation=45)
            plt.title(f'{self.best_model_name} - ç‰¹å¾´é‡é‡è¦åº¦')
            plt.ylabel('é‡è¦åº¦')
            plt.tight_layout()
            plt.savefig(self.output_dir / 'feature_importance.png', dpi=300, bbox_inches='tight')
            plt.close()
        
        logger.info("å¯è¦–åŒ–ä¿å­˜å®Œäº†")
    
    def save_results(self):
        """
        çµæœã®ä¿å­˜
        """
        logger.info("çµæœä¿å­˜ä¸­")
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # 1. ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
        model_path = self.output_dir / 'valuation_model.joblib'
        joblib.dump(self.best_model, model_path)
        
        # 2. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®ä¿å­˜
        encoders_path = self.output_dir / 'label_encoders.joblib'
        joblib.dump(self.encoders, encoders_path)
        
        # 3. è©³ç´°çµæœã®ä¿å­˜
        results_summary = {}
        for model_name, result in self.results.items():
            results_summary[model_name] = {
                'best_params': result['best_params'],
                'metrics': result['metrics'],
                'cv_mean': float(result['cv_mean']),
                'cv_std': float(result['cv_std'])
            }
        
        # 4. å­¦ç¿’æƒ…å ±ã®ä¿å­˜
        training_info = {
            'timestamp': timestamp,
            'best_model': self.best_model_name,
            'data_size': len(self.data),
            'train_size': len(self.X_train),
            'test_size': len(self.X_test),
            'feature_columns': self.feature_columns,
            'results': results_summary
        }
        
        info_path = self.output_dir / f'training_info_{timestamp}.json'
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(training_info, f, ensure_ascii=False, indent=2)
        
        # 5. æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ
        self._create_report(timestamp)
        
        logger.info(f"ä¿å­˜å®Œäº†:")
        logger.info(f"  ãƒ¢ãƒ‡ãƒ«: {model_path}")
        logger.info(f"  ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼: {encoders_path}")
        logger.info(f"  å­¦ç¿’æƒ…å ±: {info_path}")
    
    def _create_report(self, timestamp):
        """
        è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ
        """
        report_path = self.output_dir / f'model_report_{timestamp}.md'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"# ä¸å‹•ç”£æŸ»å®šãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ¬ãƒãƒ¼ãƒˆ\n\n")
            f.write(f"ä½œæˆæ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}\n\n")
            
            f.write(f"## æ¦‚è¦\n\n")
            f.write(f"- **ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«**: {self.best_model_name}\n")
            f.write(f"- **ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º**: {len(self.data):,} ä»¶\n")
            f.write(f"- **è¨“ç·´ãƒ‡ãƒ¼ã‚¿**: {len(self.X_train):,} ä»¶\n")
            f.write(f"- **ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿**: {len(self.X_test):,} ä»¶\n")
            f.write(f"- **æœ€é«˜ã‚¹ã‚³ã‚¢**: RÂ² = {self.best_score:.3f}\n\n")
            
            f.write(f"## ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒçµæœ\n\n")
            f.write(f"| ãƒ¢ãƒ‡ãƒ« | Test RÂ² | Test MAE | Test MAPE | CV MeanÂ±Std |\n")
            f.write(f"|--------|---------|----------|-----------|-------------|\n")
            
            for model_name, result in self.results.items():
                metrics = result['metrics']
                f.write(f"| {model_name} | {metrics['test_r2']:.3f} | "
                       f"Â¥{metrics['test_mae']:,.0f} | {metrics['test_mape']:.1f}% | "
                       f"{result['cv_mean']:.3f}Â±{result['cv_std']:.3f} |\n")
            
            f.write(f"\n## ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«è©³ç´°\n\n")
            best_result = self.results[self.best_model_name]
            f.write(f"**ãƒ¢ãƒ‡ãƒ«**: {self.best_model_name}\n\n")
            f.write(f"**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:\n")
            for param, value in best_result['best_params'].items():
                f.write(f"- {param}: {value}\n")
            
            f.write(f"\n**è©•ä¾¡æŒ‡æ¨™**:\n")
            metrics = best_result['metrics']
            f.write(f"- Test RÂ²: {metrics['test_r2']:.3f}\n")
            f.write(f"- Test MAE: Â¥{metrics['test_mae']:,.0f}\n")
            f.write(f"- Test RMSE: Â¥{metrics['test_rmse']:,.0f}\n")
            f.write(f"- Test MAPE: {metrics['test_mape']:.1f}%\n")
            
            f.write(f"\n## æ¨å¥¨äº‹é …\n\n")
            if self.best_score >= 0.8:
                f.write(f"âœ… **å„ªç§€**: ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã¯éå¸¸ã«é«˜ãã€æœ¬ç•ªç’°å¢ƒã§ã®ä½¿ç”¨ã«é©ã—ã¦ã„ã¾ã™ã€‚\n")
            elif self.best_score >= 0.6:
                f.write(f"ğŸ‘ **è‰¯å¥½**: ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã¯è‰¯å¥½ã§ã™ã€‚ç¶™ç¶šçš„ãªæ”¹å–„ã«ã‚ˆã‚Šæ›´ãªã‚‹å‘ä¸ŠãŒæœŸå¾…ã§ãã¾ã™ã€‚\n")
            elif self.best_score >= 0.4:
                f.write(f"âš ï¸ **æ³¨æ„**: ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã«æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚ç‰¹å¾´é‡ã®è¿½åŠ ã‚„ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã‚’æ¨å¥¨ã—ã¾ã™ã€‚\n")
            else:
                f.write(f"ğŸ”´ **è¦æ”¹å–„**: ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ãŒä½ãã€ãƒ‡ãƒ¼ã‚¿ã®è¦‹ç›´ã—ã‚„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®å¤‰æ›´ãŒå¿…è¦ã§ã™ã€‚\n")
        
        logger.info(f"ãƒ¬ãƒãƒ¼ãƒˆä½œæˆå®Œäº†: {report_path}")


def main():
    parser = argparse.ArgumentParser(description='ä¸å‹•ç”£æŸ»å®šãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ')
    parser.add_argument('--data-source', choices=['sample', 'mlit', 'csv'], 
                       default='sample', help='ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹')
    parser.add_argument('--output-dir', default='../api', 
                       help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--no-grid-search', action='store_true',
                       help='ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã‚’ã‚¹ã‚­ãƒƒãƒ—')
    parser.add_argument('--cv-folds', type=int, default=5,
                       help='äº¤å·®æ¤œè¨¼ã®åˆ†å‰²æ•°')
    parser.add_argument('--test-size', type=float, default=0.2,
                       help='ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰²åˆ')
    parser.add_argument('--random-state', type=int, default=42,
                       help='ä¹±æ•°ã‚·ãƒ¼ãƒ‰')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸ  ä¸å‹•ç”£æŸ»å®šãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: {args.data_source}")
    print(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {args.output_dir}")
    print(f"ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´: {'ç„¡åŠ¹' if args.no_grid_search else 'æœ‰åŠ¹'}")
    print(f"äº¤å·®æ¤œè¨¼åˆ†å‰²æ•°: {args.cv_folds}")
    print()
    
    try:
        # ãƒ¢ãƒ‡ãƒ«ä½œæˆå™¨ã®åˆæœŸåŒ–
        creator = ModelCreator(
            output_dir=args.output_dir,
            random_state=args.random_state
        )
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»æº–å‚™
        creator.load_and_prepare_data(
            data_source=args.data_source,
            test_size=args.test_size
        )
        
        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ»è©•ä¾¡
        creator.train_and_evaluate_models(
            use_grid_search=not args.no_grid_search,
            cv_folds=args.cv_folds
        )
        
        # å¯è¦–åŒ–ä½œæˆ
        creator.create_visualizations()
        
        # çµæœä¿å­˜
        creator.save_results()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†ï¼")
        print("=" * 60)
        print(f"ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«: {creator.best_model_name}")
        print(f"æœ€é«˜ã‚¹ã‚³ã‚¢: RÂ² = {creator.best_score:.3f}")
        print(f"å‡ºåŠ›å…ˆ: {args.output_dir}")
        
    except Exception as e:
        logger.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()