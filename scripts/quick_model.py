#!/usr/bin/env python3
"""
ã‚¯ã‚¤ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æœ€å°é™ã®è¨­å®šã§é«˜å“è³ªãªãƒ¢ãƒ‡ãƒ«ã‚’ç´ æ—©ãä½œæˆã—ã¾ã™ã€‚

ä½¿ç”¨æ–¹æ³•:
python quick_model.py [--fast|--balanced|--best]
"""

import argparse
import logging
import sys
import json
from pathlib import Path
from datetime import datetime

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error, r2_score, mean_absolute_percentage_error
import joblib

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class QuickModelCreator:
    """
    ã‚¯ã‚¤ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self, preset='balanced', output_dir='../api'):
        self.preset = preset
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.random_state = 42
        
        # ãƒ—ãƒªã‚»ãƒƒãƒˆè¨­å®š
        self.presets = {
            'fast': {
                'n_estimators': 50,
                'max_depth': 10,
                'min_samples_split': 5,
                'description': 'é«˜é€Ÿå­¦ç¿’ï¼ˆç²¾åº¦: æ™®é€šã€é€Ÿåº¦: é«˜é€Ÿï¼‰'
            },
            'balanced': {
                'n_estimators': 100,
                'max_depth': 15,
                'min_samples_split': 2,
                'description': 'ãƒãƒ©ãƒ³ã‚¹å‹ï¼ˆç²¾åº¦: è‰¯å¥½ã€é€Ÿåº¦: æ™®é€šï¼‰'
            },
            'best': {
                'n_estimators': 200,
                'max_depth': 20,
                'min_samples_split': 2,
                'description': 'é«˜ç²¾åº¦ï¼ˆç²¾åº¦: æœ€é«˜ã€é€Ÿåº¦: ä½é€Ÿï¼‰'
            }
        }
    
    def generate_sample_data(self, n_samples=1500):
        """
        é«˜å“è³ªã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
        """
        np.random.seed(self.random_state)
        
        # ã‚ˆã‚Šç¾å®Ÿçš„ãªåœ°åŸŸè¨­å®š
        prefectures = ['æ±äº¬éƒ½', 'ç¥å¥ˆå·çœŒ', 'å¤§é˜ªåºœ', 'æ„›çŸ¥çœŒ', 'åŸ¼ç‰çœŒ', 'åƒè‘‰çœŒ']
        cities = {
            'æ±äº¬éƒ½': ['æ¸‹è°·åŒº', 'æ–°å®¿åŒº', 'æ¸¯åŒº', 'åƒä»£ç”°åŒº', 'ä¸­å¤®åŒº', 'ä¸–ç”°è°·åŒº', 'ç›®é»’åŒº'],
            'ç¥å¥ˆå·çœŒ': ['æ¨ªæµœå¸‚', 'å·å´å¸‚', 'ç›¸æ¨¡åŸå¸‚', 'è—¤æ²¢å¸‚'],
            'å¤§é˜ªåºœ': ['å¤§é˜ªå¸‚', 'å ºå¸‚', 'æ±å¤§é˜ªå¸‚', 'è±Šä¸­å¸‚'],
            'æ„›çŸ¥çœŒ': ['åå¤å±‹å¸‚', 'è±Šç”°å¸‚', 'å²¡å´å¸‚'],
            'åŸ¼ç‰çœŒ': ['ã•ã„ãŸã¾å¸‚', 'å·å£å¸‚', 'æ‰€æ²¢å¸‚'],
            'åƒè‘‰çœŒ': ['åƒè‘‰å¸‚', 'èˆ¹æ©‹å¸‚', 'æŸå¸‚']
        }
        
        districts = ['ä¸­å¤®', 'æ±', 'è¥¿', 'å—', 'åŒ—', 'é§…å‰', 'æ–°ç”º', 'æœ¬ç”º']
        
        # åœ°åŸŸåˆ¥ä¾¡æ ¼ä¿‚æ•°ï¼ˆã‚ˆã‚Šç¾å®Ÿçš„ï¼‰
        price_multipliers = {
            'æ±äº¬éƒ½': {'æ¸‹è°·åŒº': 2.8, 'æ–°å®¿åŒº': 2.5, 'æ¸¯åŒº': 3.2, 'åƒä»£ç”°åŒº': 3.5, 
                     'ä¸­å¤®åŒº': 3.0, 'ä¸–ç”°è°·åŒº': 2.2, 'ç›®é»’åŒº': 2.4},
            'ç¥å¥ˆå·çœŒ': {'æ¨ªæµœå¸‚': 1.8, 'å·å´å¸‚': 1.6, 'ç›¸æ¨¡åŸå¸‚': 1.3, 'è—¤æ²¢å¸‚': 1.5},
            'å¤§é˜ªåºœ': {'å¤§é˜ªå¸‚': 1.7, 'å ºå¸‚': 1.3, 'æ±å¤§é˜ªå¸‚': 1.2, 'è±Šä¸­å¸‚': 1.4},
            'æ„›çŸ¥çœŒ': {'åå¤å±‹å¸‚': 1.5, 'è±Šç”°å¸‚': 1.2, 'å²¡å´å¸‚': 1.1},
            'åŸ¼ç‰çœŒ': {'ã•ã„ãŸã¾å¸‚': 1.4, 'å·å£å¸‚': 1.3, 'æ‰€æ²¢å¸‚': 1.2},
            'åƒè‘‰çœŒ': {'åƒè‘‰å¸‚': 1.3, 'èˆ¹æ©‹å¸‚': 1.4, 'æŸå¸‚': 1.2}
        }
        
        data = []
        
        for _ in range(n_samples):
            prefecture = np.random.choice(prefectures)
            city = np.random.choice(list(cities[prefecture]))
            district = np.random.choice(districts)
            
            # é¢ç©ã®ç”Ÿæˆï¼ˆå¯¾æ•°æ­£è¦åˆ†å¸ƒï¼‰
            land_area = np.random.lognormal(4.6, 0.5)  # å¹³å‡ç´„100ã¡
            building_area = land_area * np.random.uniform(0.5, 1.1)
            
            # ç¯‰å¹´æ•°ï¼ˆãƒ¯ã‚¤ãƒ–ãƒ«åˆ†å¸ƒã§ç¾å®Ÿçš„ãªåˆ†å¸ƒï¼‰
            building_age = np.random.weibull(2) * 25
            building_age = min(building_age, 50)
            
            # ä¾¡æ ¼è¨ˆç®—ï¼ˆã‚ˆã‚Šç²¾å¯†ãªè¨ˆç®—å¼ï¼‰
            base_price_per_sqm = 400000  # åŸºæº–ä¾¡æ ¼
            
            # åœ°åŸŸä¿‚æ•°
            location_factor = price_multipliers[prefecture][city]
            location_factor *= np.random.uniform(0.9, 1.1)  # åœ°åŸŸå†…ã®ã°ã‚‰ã¤ã
            
            # é¢ç©åŠ¹æœ
            area_factor = np.sqrt(land_area + building_area * 0.8)
            
            # ç¯‰å¹´åŠ£åŒ–
            age_factor = max(0.4, np.exp(-building_age * 0.03))
            
            # å¸‚å ´ãƒ©ãƒ³ãƒ€ãƒ è¦å› 
            market_factor = np.random.lognormal(0, 0.2)
            
            price = (base_price_per_sqm * area_factor * location_factor * 
                    age_factor * market_factor)
            
            # ç‰¹æ®Šç‰©ä»¶ï¼ˆ5%ã®ç¢ºç‡ï¼‰
            if np.random.random() < 0.05:
                price *= np.random.uniform(1.5, 2.5)
            
            data.append({
                'éƒ½é“åºœçœŒ': prefecture,
                'å¸‚åŒºç”ºæ‘': city,
                'åœ°åŒº': district,
                'åœŸåœ°é¢ç©': max(25, land_area),
                'å»ºç‰©é¢ç©': max(20, building_area),
                'ç¯‰å¹´æ•°': int(building_age),
                'å–å¼•ä¾¡æ ¼': int(price)
            })
        
        return pd.DataFrame(data)
    
    def preprocess_data(self, df):
        """
        ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        """
        logger.info("ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ä¸­...")
        
        df_clean = df.copy()
        original_count = len(df_clean)
        
        # ç•°å¸¸å€¤é™¤å»
        df_clean = df_clean[
            (df_clean['å–å¼•ä¾¡æ ¼'] > 5_000_000) &
            (df_clean['å–å¼•ä¾¡æ ¼'] < 2_000_000_000) &
            (df_clean['åœŸåœ°é¢ç©'] > 15) &
            (df_clean['åœŸåœ°é¢ç©'] < 1000) &
            (df_clean['å»ºç‰©é¢ç©'] > 15) &
            (df_clean['å»ºç‰©é¢ç©'] < 800) &
            (df_clean['ç¯‰å¹´æ•°'] >= 0) &
            (df_clean['ç¯‰å¹´æ•°'] <= 60)
        ]
        
        # æ¬ æå€¤é™¤å»
        df_clean = df_clean.dropna()
        
        removed_count = original_count - len(df_clean)
        logger.info(f"å‰å‡¦ç†å®Œäº†: {removed_count} ä»¶å‰Šé™¤ ({removed_count/original_count*100:.1f}%)")
        
        return df_clean
    
    def prepare_features(self, df):
        """
        ç‰¹å¾´é‡æº–å‚™
        """
        feature_columns = ['éƒ½é“åºœçœŒ', 'å¸‚åŒºç”ºæ‘', 'åœ°åŒº', 'åœŸåœ°é¢ç©', 'å»ºç‰©é¢ç©', 'ç¯‰å¹´æ•°']
        X = df[feature_columns].copy()
        y = df['å–å¼•ä¾¡æ ¼']
        
        # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        categorical_columns = ['éƒ½é“åºœçœŒ', 'å¸‚åŒºç”ºæ‘', 'åœ°åŒº']
        self.encoders = {}
        
        for col in categorical_columns:
            encoder = LabelEncoder()
            X[col] = encoder.fit_transform(X[col].astype(str))
            self.encoders[col] = encoder
        
        return X, y
    
    def create_and_train_model(self, X, y):
        """
        ãƒ¢ãƒ‡ãƒ«ä½œæˆãƒ»å­¦ç¿’
        """
        config = self.presets[self.preset]
        logger.info(f"ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­: {config['description']}")
        
        # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
        model = RandomForestRegressor(
            n_estimators=config['n_estimators'],
            max_depth=config['max_depth'],
            min_samples_split=config['min_samples_split'],
            random_state=self.random_state,
            n_jobs=-1
        )
        
        # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=self.random_state
        )
        
        # å­¦ç¿’
        model.fit(X_train, y_train)
        
        # è©•ä¾¡
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)
        
        metrics = {
            'train_r2': r2_score(y_train, y_train_pred),
            'test_r2': r2_score(y_test, y_test_pred),
            'test_mae': mean_absolute_error(y_test, y_test_pred),
            'test_mape': mean_absolute_percentage_error(y_test, y_test_pred) * 100
        }
        
        # äº¤å·®æ¤œè¨¼
        cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')
        metrics['cv_mean'] = cv_scores.mean()
        metrics['cv_std'] = cv_scores.std()
        
        return model, metrics, (X_train, X_test, y_train, y_test, y_test_pred)
    
    def save_model(self, model, metrics, data_info):
        """
        ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        """
        logger.info("ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­...")
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        model_path = self.output_dir / 'valuation_model.joblib'
        joblib.dump(model, model_path)
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ä¿å­˜
        encoders_path = self.output_dir / 'label_encoders.joblib'
        joblib.dump(self.encoders, encoders_path)
        
        # å­¦ç¿’æƒ…å ±ä¿å­˜
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        training_info = {
            'timestamp': timestamp,
            'preset': self.preset,
            'preset_description': self.presets[self.preset]['description'],
            'model_params': self.presets[self.preset],
            'metrics': {k: float(v) for k, v in metrics.items()},
            'data_info': data_info
        }
        
        info_path = self.output_dir / f'quick_model_info_{timestamp}.json'
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(training_info, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ä¿å­˜å®Œäº†:")
        logger.info(f"  ãƒ¢ãƒ‡ãƒ«: {model_path}")
        logger.info(f"  ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼: {encoders_path}")
        logger.info(f"  æƒ…å ±: {info_path}")
        
        return model_path, encoders_path, info_path
    
    def run(self):
        """
        ã‚¯ã‚¤ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ä½œæˆã®å®Ÿè¡Œ
        """
        print("=" * 60)
        print("âš¡ ã‚¯ã‚¤ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ä½œæˆ")
        print("=" * 60)
        print(f"ãƒ—ãƒªã‚»ãƒƒãƒˆ: {self.preset}")
        print(f"èª¬æ˜: {self.presets[self.preset]['description']}")
        print()
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            logger.info("ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...")
            df = self.generate_sample_data()
            logger.info(f"ç”Ÿæˆå®Œäº†: {len(df)} ä»¶")
            
            # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
            df_clean = self.preprocess_data(df)
            
            # ç‰¹å¾´é‡æº–å‚™
            X, y = self.prepare_features(df_clean)
            
            # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
            model, metrics, split_data = self.create_and_train_model(X, y)
            
            # çµæœè¡¨ç¤º
            print("ğŸ“Š å­¦ç¿’çµæœ:")
            print("-" * 40)
            print(f"Test RÂ² Score:  {metrics['test_r2']:.3f}")
            print(f"Test MAE:       Â¥{metrics['test_mae']:,.0f}")
            print(f"Test MAPE:      {metrics['test_mape']:.1f}%")
            print(f"CV RÂ² Score:    {metrics['cv_mean']:.3f} Â± {metrics['cv_std']:.3f}")
            print()
            
            # ãƒ‡ãƒ¼ã‚¿æƒ…å ±
            data_info = {
                'total_samples': len(df_clean),
                'train_samples': len(split_data[0]),
                'test_samples': len(split_data[1])
            }
            
            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            paths = self.save_model(model, metrics, data_info)
            
            # æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            if metrics['test_r2'] >= 0.7:
                status = "ğŸŒŸ å„ªç§€"
                message = "é«˜ç²¾åº¦ãªãƒ¢ãƒ‡ãƒ«ãŒä½œæˆã•ã‚Œã¾ã—ãŸï¼"
            elif metrics['test_r2'] >= 0.5:
                status = "ğŸ‘ è‰¯å¥½"
                message = "å®Ÿç”¨çš„ãªãƒ¢ãƒ‡ãƒ«ãŒä½œæˆã•ã‚Œã¾ã—ãŸã€‚"
            else:
                status = "âš ï¸ æ³¨æ„"
                message = "ç²¾åº¦ã«ã‚„ã‚„æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚"
            
            print("=" * 60)
            print(f"ğŸ‰ ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†ï¼")
            print("=" * 60)
            print(f"è©•ä¾¡: {status}")
            print(f"ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {message}")
            print()
            print("ğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
            print("1. docker-compose restart api  # APIã‚µãƒ¼ãƒãƒ¼å†èµ·å‹•")
            print("2. python test_model_accuracy.py  # ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ")
            print("3. ./manage_model.sh evaluate  # è©³ç´°è©•ä¾¡")
            
            return True
            
        except Exception as e:
            logger.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return False


def main():
    parser = argparse.ArgumentParser(description='ã‚¯ã‚¤ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ')
    parser.add_argument('--preset', choices=['fast', 'balanced', 'best'], 
                       default='balanced', help='å­¦ç¿’ãƒ—ãƒªã‚»ãƒƒãƒˆ')
    parser.add_argument('--output-dir', default='../api', 
                       help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    
    args = parser.parse_args()
    
    # ã‚¯ã‚¤ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Ÿè¡Œ
    creator = QuickModelCreator(
        preset=args.preset,
        output_dir=args.output_dir
    )
    
    success = creator.run()
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()