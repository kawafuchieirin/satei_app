#!/usr/bin/env python3
"""
ãƒãƒƒãƒå‡¦ç†ç”¨ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

è¤‡æ•°ã®è¨­å®šã§ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã€æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•é¸æŠã—ã¾ã™ã€‚

ä½¿ç”¨æ–¹æ³•:
python batch_model_training.py [options]
"""

import argparse
import logging
import sys
import json
import time
from pathlib import Path
from datetime import datetime
import concurrent.futures
import multiprocessing

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge, Lasso
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error, r2_score
import joblib

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class BatchModelTrainer:
    """
    ãƒãƒƒãƒå‡¦ç†ç”¨ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self, output_dir='../api', random_state=42):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.random_state = random_state
        self.results = []
        self.best_config = None
        self.best_score = -np.inf
        
        # CPUã‚³ã‚¢æ•°ã®å–å¾—
        self.n_jobs = max(1, multiprocessing.cpu_count() - 1)
        
    def generate_model_configurations(self):
        """
        ãƒ¢ãƒ‡ãƒ«è¨­å®šã®ãƒãƒƒãƒç”Ÿæˆ
        """
        configurations = []
        
        # Random Forest ã®è¨­å®šãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
        rf_configs = [
            {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 5},
            {'n_estimators': 200, 'max_depth': 15, 'min_samples_split': 2},
            {'n_estimators': 300, 'max_depth': 20, 'min_samples_split': 10},
            {'n_estimators': 150, 'max_depth': None, 'min_samples_split': 5},
        ]
        
        for config in rf_configs:
            configurations.append({
                'name': f"rf_{config['n_estimators']}_{config['max_depth']}_{config['min_samples_split']}",
                'model_type': 'random_forest',
                'model': RandomForestRegressor(
                    random_state=self.random_state,
                    n_jobs=self.n_jobs,
                    **config
                ),
                'params': config
            })
        
        # Gradient Boosting ã®è¨­å®šãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
        gb_configs = [
            {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 5},
            {'n_estimators': 200, 'learning_rate': 0.05, 'max_depth': 3},
            {'n_estimators': 150, 'learning_rate': 0.15, 'max_depth': 7},
        ]
        
        for config in gb_configs:
            configurations.append({
                'name': f"gb_{config['n_estimators']}_{config['learning_rate']}_{config['max_depth']}",
                'model_type': 'gradient_boosting',
                'model': GradientBoostingRegressor(
                    random_state=self.random_state,
                    **config
                ),
                'params': config
            })
        
        # Ridgeå›å¸°ã®è¨­å®šãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
        ridge_alphas = [0.1, 1.0, 10.0, 100.0]
        for alpha in ridge_alphas:
            configurations.append({
                'name': f"ridge_{alpha}",
                'model_type': 'ridge',
                'model': Pipeline([
                    ('scaler', StandardScaler()),
                    ('regressor', Ridge(alpha=alpha, random_state=self.random_state))
                ]),
                'params': {'alpha': alpha}
            })
        
        # Lassoå›å¸°ã®è¨­å®šãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
        lasso_alphas = [0.1, 1.0, 10.0]
        for alpha in lasso_alphas:
            configurations.append({
                'name': f"lasso_{alpha}",
                'model_type': 'lasso',
                'model': Pipeline([
                    ('scaler', StandardScaler()),
                    ('regressor', Lasso(alpha=alpha, random_state=self.random_state))
                ]),
                'params': {'alpha': alpha}
            })
        
        return configurations
    
    def load_and_prepare_data(self, data_source='sample'):
        """
        ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨æº–å‚™
        """
        logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹: {data_source}")
        
        if data_source == 'sample':
            self.data = self._generate_sample_data()
        elif data_source == 'mlit':
            self.data = self._load_mlit_data()
        elif data_source == 'csv':
            self.data = self._load_csv_data()
        
        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        self.data = self._preprocess_data(self.data)
        
        # ç‰¹å¾´é‡æº–å‚™
        feature_columns = ['éƒ½é“åºœçœŒ', 'å¸‚åŒºç”ºæ‘', 'åœ°åŒº', 'åœŸåœ°é¢ç©', 'å»ºç‰©é¢ç©', 'ç¯‰å¹´æ•°']
        X = self.data[feature_columns].copy()
        y = self.data['å–å¼•ä¾¡æ ¼']
        
        # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        categorical_columns = ['éƒ½é“åºœçœŒ', 'å¸‚åŒºç”ºæ‘', 'åœ°åŒº']
        self.encoders = {}
        
        for col in categorical_columns:
            if col in X.columns:
                encoder = LabelEncoder()
                X[col] = encoder.fit_transform(X[col].astype(str))
                self.encoders[col] = encoder
        
        # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=0.2, random_state=self.random_state
        )
        
        logger.info(f"ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº† - è¨“ç·´: {len(self.X_train)}, ãƒ†ã‚¹ãƒˆ: {len(self.X_test)}")
    
    def _generate_sample_data(self, n_samples=2000):
        """
        ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        """
        np.random.seed(self.random_state)
        
        prefectures = ['æ±äº¬éƒ½', 'ç¥å¥ˆå·çœŒ', 'å¤§é˜ªåºœ', 'æ„›çŸ¥çœŒ', 'åŸ¼ç‰çœŒ']
        cities = ['æ¸‹è°·åŒº', 'æ–°å®¿åŒº', 'æ¸¯åŒº', 'æ¨ªæµœå¸‚', 'å·å´å¸‚', 'å¤§é˜ªå¸‚', 'åå¤å±‹å¸‚', 'ã•ã„ãŸã¾å¸‚']
        districts = ['ä¸­å¤®', 'æ±', 'è¥¿', 'å—', 'åŒ—', 'é§…å‰']
        
        data = []
        for _ in range(n_samples):
            prefecture = np.random.choice(prefectures)
            city = np.random.choice(cities)
            district = np.random.choice(districts)
            
            land_area = np.random.lognormal(4.5, 0.6)
            building_area = land_area * np.random.uniform(0.6, 1.2)
            building_age = np.random.exponential(15)
            building_age = min(building_age, 50)
            
            # ä¾¡æ ¼è¨ˆç®—
            base_price = 300000
            location_factor = np.random.uniform(0.8, 2.5)
            age_factor = max(0.3, 1.0 - building_age * 0.015)
            price = base_price * (land_area * 0.7 + building_area * 0.3) * location_factor * age_factor
            
            data.append({
                'éƒ½é“åºœçœŒ': prefecture,
                'å¸‚åŒºç”ºæ‘': city,
                'åœ°åŒº': district,
                'åœŸåœ°é¢ç©': max(20, land_area),
                'å»ºç‰©é¢ç©': max(15, building_area),
                'ç¯‰å¹´æ•°': int(building_age),
                'å–å¼•ä¾¡æ ¼': int(price)
            })
        
        return pd.DataFrame(data)
    
    def _load_mlit_data(self):
        """
        MLIT APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        """
        try:
            sys.path.append(str(Path(__file__).parent.parent / 'api'))
            from models.data_fetcher import MLITDataFetcher
            
            fetcher = MLITDataFetcher()
            df = fetcher.fetch_trade_data(prefecture="æ±äº¬éƒ½", from_year=2021, to_year=2024)
            
            if df.empty:
                logger.warning("MLIT APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                return self._generate_sample_data()
            
            return df
        except Exception as e:
            logger.error(f"MLIT API ã‚¨ãƒ©ãƒ¼: {e}")
            return self._generate_sample_data()
    
    def _load_csv_data(self):
        """
        CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        """
        csv_path = self.output_dir / 'training_data.csv'
        if csv_path.exists():
            return pd.read_csv(csv_path)
        else:
            logger.warning(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}")
            return self._generate_sample_data()
    
    def _preprocess_data(self, df):
        """
        ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        """
        df_clean = df.copy()
        
        # åˆ—åçµ±ä¸€
        column_mapping = {
            'å–å¼•ä¾¡æ ¼ï¼ˆç·é¡ï¼‰': 'å–å¼•ä¾¡æ ¼',
            'é¢ç©ï¼ˆã¡ï¼‰': 'åœŸåœ°é¢ç©'
        }
        df_clean = df_clean.rename(columns=column_mapping)
        
        # æ•°å€¤å¤‰æ›
        numeric_columns = ['åœŸåœ°é¢ç©', 'å»ºç‰©é¢ç©', 'ç¯‰å¹´æ•°', 'å–å¼•ä¾¡æ ¼']
        for col in numeric_columns:
            if col in df_clean.columns:
                df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
        
        # ç•°å¸¸å€¤é™¤å»
        df_clean = df_clean[
            (df_clean['å–å¼•ä¾¡æ ¼'] > 1_000_000) &
            (df_clean['å–å¼•ä¾¡æ ¼'] < 3_000_000_000) &
            (df_clean['åœŸåœ°é¢ç©'] > 10) &
            (df_clean['åœŸåœ°é¢ç©'] < 2000) &
            (df_clean['å»ºç‰©é¢ç©'] > 10) &
            (df_clean['å»ºç‰©é¢ç©'] < 1000) &
            (df_clean['ç¯‰å¹´æ•°'] >= 0) &
            (df_clean['ç¯‰å¹´æ•°'] <= 80)
        ]
        
        # æ¬ æå€¤é™¤å»
        df_clean = df_clean.dropna()
        
        return df_clean
    
    def train_single_model(self, config):
        """
        å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ï¼ˆä¸¦åˆ—å‡¦ç†ç”¨ï¼‰
        """
        try:
            start_time = time.time()
            
            # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
            model = config['model']
            model.fit(self.X_train, self.y_train)
            
            # äºˆæ¸¬
            y_train_pred = model.predict(self.X_train)
            y_test_pred = model.predict(self.X_test)
            
            # è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
            train_r2 = r2_score(self.y_train, y_train_pred)
            test_r2 = r2_score(self.y_test, y_test_pred)
            test_mae = mean_absolute_error(self.y_test, y_test_pred)
            
            # äº¤å·®æ¤œè¨¼
            cv_scores = cross_val_score(model, self.X_train, self.y_train, cv=3, scoring='r2')
            
            training_time = time.time() - start_time
            
            result = {
                'name': config['name'],
                'model_type': config['model_type'],
                'params': config['params'],
                'model': model,
                'metrics': {
                    'train_r2': train_r2,
                    'test_r2': test_r2,
                    'test_mae': test_mae,
                    'cv_mean': cv_scores.mean(),
                    'cv_std': cv_scores.std()
                },
                'training_time': training_time
            }
            
            logger.info(f"{config['name']} å®Œäº† - Test RÂ²: {test_r2:.3f}, æ™‚é–“: {training_time:.1f}s")
            return result
            
        except Exception as e:
            logger.error(f"{config['name']} ã§ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def run_batch_training(self, max_workers=None):
        """
        ãƒãƒƒãƒå­¦ç¿’ã®å®Ÿè¡Œ
        """
        logger.info("ãƒãƒƒãƒå­¦ç¿’é–‹å§‹")
        
        configurations = self.generate_model_configurations()
        logger.info(f"å­¦ç¿’äºˆå®šãƒ¢ãƒ‡ãƒ«æ•°: {len(configurations)}")
        
        if max_workers is None:
            max_workers = min(self.n_jobs, len(configurations))
        
        # ä¸¦åˆ—å­¦ç¿’å®Ÿè¡Œ
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # å…¨ã¦ã®è¨­å®šã‚’ä¸¦åˆ—ã§å®Ÿè¡Œ
            future_to_config = {
                executor.submit(self.train_single_model, config): config 
                for config in configurations
            }
            
            # çµæœåé›†
            for future in concurrent.futures.as_completed(future_to_config):
                result = future.result()
                if result is not None:
                    self.results.append(result)
                    
                    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«æ›´æ–°
                    if result['metrics']['test_r2'] > self.best_score:
                        self.best_score = result['metrics']['test_r2']
                        self.best_config = result
        
        # çµæœã‚½ãƒ¼ãƒˆ
        self.results.sort(key=lambda x: x['metrics']['test_r2'], reverse=True)
        
        logger.info(f"ãƒãƒƒãƒå­¦ç¿’å®Œäº† - ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«: {self.best_config['name']} (RÂ² = {self.best_score:.3f})")
    
    def save_best_model(self):
        """
        æœ€å„ªç§€ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
        """
        if self.best_config is None:
            logger.error("ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        logger.info("ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­")
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        model_path = self.output_dir / 'valuation_model.joblib'
        joblib.dump(self.best_config['model'], model_path)
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ä¿å­˜
        encoders_path = self.output_dir / 'label_encoders.joblib'
        joblib.dump(self.encoders, encoders_path)
        
        # å­¦ç¿’æƒ…å ±ä¿å­˜
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        training_info = {
            'timestamp': timestamp,
            'best_model': {
                'name': self.best_config['name'],
                'type': self.best_config['model_type'],
                'params': self.best_config['params'],
                'metrics': self.best_config['metrics'],
                'training_time': self.best_config['training_time']
            },
            'all_results': [
                {
                    'name': result['name'],
                    'type': result['model_type'],
                    'metrics': result['metrics'],
                    'training_time': result['training_time']
                }
                for result in self.results
            ],
            'data_info': {
                'total_samples': len(self.data),
                'train_samples': len(self.X_train),
                'test_samples': len(self.X_test)
            }
        }
        
        info_path = self.output_dir / f'batch_training_results_{timestamp}.json'
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(training_info, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ä¿å­˜å®Œäº†:")
        logger.info(f"  ãƒ¢ãƒ‡ãƒ«: {model_path}")
        logger.info(f"  ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼: {encoders_path}")
        logger.info(f"  çµæœ: {info_path}")
    
    def print_results_summary(self):
        """
        çµæœã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º
        """
        print("\n" + "=" * 80)
        print("ğŸ† ãƒãƒƒãƒå­¦ç¿’çµæœã‚µãƒãƒªãƒ¼")
        print("=" * 80)
        
        print(f"å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«æ•°: {len(self.results)}")
        print(f"ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«: {self.best_config['name']}")
        print(f"æœ€é«˜ã‚¹ã‚³ã‚¢: RÂ² = {self.best_score:.3f}")
        print()
        
        print("ğŸ“Š ä¸Šä½5ãƒ¢ãƒ‡ãƒ«:")
        print("-" * 80)
        print(f"{'é †ä½':<4} {'ãƒ¢ãƒ‡ãƒ«å':<20} {'Test RÂ²':<10} {'Test MAE':<15} {'å­¦ç¿’æ™‚é–“':<10}")
        print("-" * 80)
        
        for i, result in enumerate(self.results[:5], 1):
            metrics = result['metrics']
            print(f"{i:<4} {result['name']:<20} {metrics['test_r2']:<10.3f} "
                  f"Â¥{metrics['test_mae']:<13,.0f} {result['training_time']:<10.1f}s")
        
        print()
        
        # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ
        type_stats = {}
        for result in self.results:
            model_type = result['model_type']
            if model_type not in type_stats:
                type_stats[model_type] = []
            type_stats[model_type].append(result['metrics']['test_r2'])
        
        print("ğŸ“ˆ ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ:")
        print("-" * 50)
        for model_type, scores in type_stats.items():
            avg_score = np.mean(scores)
            max_score = np.max(scores)
            print(f"{model_type:<20} å¹³å‡: {avg_score:.3f}, æœ€é«˜: {max_score:.3f}")


def main():
    parser = argparse.ArgumentParser(description='ãƒãƒƒãƒå‡¦ç†ç”¨ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ')
    parser.add_argument('--data-source', choices=['sample', 'mlit', 'csv'], 
                       default='sample', help='ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹')
    parser.add_argument('--output-dir', default='../api', 
                       help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--max-workers', type=int, default=None,
                       help='ä¸¦åˆ—å‡¦ç†ã®æœ€å¤§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°')
    parser.add_argument('--random-state', type=int, default=42,
                       help='ä¹±æ•°ã‚·ãƒ¼ãƒ‰')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸš€ ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: {args.data_source}")
    print(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {args.output_dir}")
    print(f"æœ€å¤§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {args.max_workers or 'auto'}")
    print()
    
    try:
        # ãƒãƒƒãƒå­¦ç¿’å™¨ã®åˆæœŸåŒ–
        trainer = BatchModelTrainer(
            output_dir=args.output_dir,
            random_state=args.random_state
        )
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        trainer.load_and_prepare_data(data_source=args.data_source)
        
        # ãƒãƒƒãƒå­¦ç¿’å®Ÿè¡Œ
        trainer.run_batch_training(max_workers=args.max_workers)
        
        # çµæœè¡¨ç¤º
        trainer.print_results_summary()
        
        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜
        trainer.save_best_model()
        
        print("\nğŸ‰ ãƒãƒƒãƒå­¦ç¿’å®Œäº†ï¼")
        
    except Exception as e:
        logger.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()