#!/usr/bin/env python3
"""
ä¸å‹•ç”£æŸ»å®šãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import json
import sys
import logging
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).parent))

from models.model_evaluator import ModelEvaluator

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def main():
    """
    ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
    """
    print("=" * 60)
    print("ä¸å‹•ç”£æŸ»å®šãƒ¢ãƒ‡ãƒ« ç²¾åº¦æ¤œè¨¼")
    print("=" * 60)
    
    try:
        # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡å™¨ã®åˆæœŸåŒ–
        evaluator = ModelEvaluator()
        
        print("\n1. ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ä¸­...")
        evaluator.load_model_and_data()
        print(f"   ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(evaluator.data):,}ä»¶")
        
        print("\n2. ãƒ¢ãƒ‡ãƒ«æ€§èƒ½è©•ä¾¡ä¸­...")
        performance = evaluator.evaluate_model_performance()
        
        print("\n3. äº¤å·®æ¤œè¨¼å®Ÿè¡Œä¸­...")
        cv_results = evaluator.cross_validate_model(cv_folds=5)
        
        print("\n4. äºˆæ¸¬ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆä¸­...")
        samples = evaluator.generate_prediction_samples(n_samples=5)
        
        # çµæœè¡¨ç¤º
        print_evaluation_results(performance, cv_results, samples)
        
        # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        save_results_to_file(performance, cv_results, samples)
        
    except Exception as e:
        logger.error(f"è©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        sys.exit(1)


def print_evaluation_results(performance: dict, cv_results: dict, samples: list):
    """
    è©•ä¾¡çµæœã®è¡¨ç¤º
    """
    print("\n" + "=" * 60)
    print("è©•ä¾¡çµæœ")
    print("=" * 60)
    
    # å…¨ä½“çš„ãªæ€§èƒ½æŒ‡æ¨™
    metrics = performance['overall_metrics']
    print(f"\nğŸ“Š å…¨ä½“çš„ãªæ€§èƒ½æŒ‡æ¨™:")
    print(f"   RÂ² Score (æ±ºå®šä¿‚æ•°):     {metrics['r2_score']:.3f}")
    print(f"   MAE (å¹³å‡çµ¶å¯¾èª¤å·®):      Â¥{metrics['mae']:,.0f}")
    print(f"   RMSE (äºŒä¹—å¹³å‡å¹³æ–¹æ ¹èª¤å·®): Â¥{metrics['rmse']:,.0f}")
    print(f"   MAPE (å¹³å‡çµ¶å¯¾ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆèª¤å·®): {metrics['mape']:.1f}%")
    
    # äº¤å·®æ¤œè¨¼çµæœ
    print(f"\nğŸ”„ äº¤å·®æ¤œè¨¼çµæœ ({cv_results['cv_folds']}åˆ†å‰²):")
    print(f"   RÂ² Score: {cv_results['r2_mean']:.3f} Â± {cv_results['r2_std']:.3f}")
    print(f"   MAE:      Â¥{cv_results['mae_mean']:,.0f} Â± Â¥{cv_results['mae_std']:,.0f}")
    
    # ä¾¡æ ¼å¸¯åˆ¥ç²¾åº¦
    if performance['price_range_accuracy']:
        print(f"\nğŸ’° ä¾¡æ ¼å¸¯åˆ¥ç²¾åº¦:")
        for price_range, accuracy in performance['price_range_accuracy'].items():
            print(f"   {price_range}:")
            print(f"     ä»¶æ•°: {accuracy['count']:,}ä»¶")
            print(f"     RÂ² Score: {accuracy['r2_score']:.3f}")
            print(f"     MAE: Â¥{accuracy['mae']:,.0f}")
            print(f"     MAPE: {accuracy['mape']:.1f}%")
    
    # ç‰¹å¾´é‡é‡è¦åº¦
    if performance['feature_importance']['features']:
        print(f"\nğŸ” ç‰¹å¾´é‡é‡è¦åº¦ (ä¸Šä½5ä½):")
        features = performance['feature_importance']['features'][:5]
        importance = performance['feature_importance']['importance'][:5]
        for i, (feature, imp) in enumerate(zip(features, importance), 1):
            print(f"   {i}. {feature}: {imp:.3f}")
    
    # è©•ä¾¡ã‚µãƒãƒªãƒ¼
    print(f"\nğŸ“ è©•ä¾¡ã‚µãƒãƒªãƒ¼:")
    for summary in performance['evaluation_summary']:
        print(f"   â€¢ {summary}")
    
    # äºˆæ¸¬ã‚µãƒ³ãƒ—ãƒ«
    if samples:
        print(f"\nğŸ¯ äºˆæ¸¬ã‚µãƒ³ãƒ—ãƒ« (å®Ÿéš›ã®å–å¼•ãƒ‡ãƒ¼ã‚¿ã¨ã®æ¯”è¼ƒ):")
        for i, sample in enumerate(samples[:3], 1):
            input_data = sample['input']
            print(f"\n   ã‚µãƒ³ãƒ—ãƒ« {i}:")
            print(f"     æ‰€åœ¨åœ°: {input_data['prefecture']} {input_data['city']} {input_data['district']}")
            print(f"     åœŸåœ°é¢ç©: {input_data['land_area']:.1f}ã¡")
            print(f"     å»ºç‰©é¢ç©: {input_data['building_area']:.1f}ã¡")
            print(f"     ç¯‰å¹´æ•°: {input_data['building_age']}å¹´")
            print(f"     å®Ÿéš›ã®ä¾¡æ ¼: Â¥{sample['actual_price']:,.0f}")
            print(f"     äºˆæ¸¬ä¾¡æ ¼:   Â¥{sample['predicted_price']:,.0f}")
            print(f"     èª¤å·®ç‡:     {sample['error_rate']:.1f}%")


def save_results_to_file(performance: dict, cv_results: dict, samples: list):
    """
    çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    """
    results = {
        'model_performance': performance,
        'cross_validation': cv_results,
        'prediction_samples': samples
    }
    
    output_file = Path(__file__).parent / 'model_evaluation_results.json'
    
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ è©•ä¾¡çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")
        
    except Exception as e:
        logger.warning(f"çµæœã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")


if __name__ == "__main__":
    main()