#!/usr/bin/env python
"""
Dockeräº’æ›ãƒ¢ãƒ‡ãƒ«ä¿®æ­£ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
NumPy 1.24.3ç’°å¢ƒã¨äº’æ›æ€§ã®ã‚ã‚‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ»ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’ä½œæˆ
"""

import os
import logging
import joblib
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
import json

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def create_minimal_compatible_files():
    """æœ€å°é™ã®äº’æ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
    
    logger.info("Creating Docker-compatible encoder and scaler files...")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    output_dir = '/Users/kawabuchieirin/Desktop/satei_app/model-creation/models'
    
    # 1. äº’æ›æ€§ã®ã‚ã‚‹ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ä½œæˆ
    logger.info("Creating label encoders...")
    
    # æ±äº¬23åŒºã®ä¸»è¦åŒºã¨ã‚µãƒ³ãƒ—ãƒ«åœ°åŒº
    municipalities = ['æ¸‹è°·åŒº', 'æ–°å®¿åŒº', 'æ¸¯åŒº', 'ä¸­å¤®åŒº', 'åƒä»£ç”°åŒº', 'å“å·åŒº', 
                     'ç›®é»’åŒº', 'ä¸–ç”°è°·åŒº', 'å¤§ç”°åŒº', 'æ±Ÿæ±åŒº', 'æ–‡äº¬åŒº', 'å°æ±åŒº']
    districts = ['', '1ä¸ç›®', '2ä¸ç›®', '3ä¸ç›®', '4ä¸ç›®', '5ä¸ç›®', 'è¥¿', 'æ±', 'å—', 'åŒ—']
    
    # Municipality encoder
    municipality_encoder = LabelEncoder()
    municipality_encoder.fit(municipalities)
    
    # District encoder  
    district_encoder = LabelEncoder()
    district_encoder.fit(districts)
    
    # æ—¥æœ¬èªã‚­ãƒ¼ã§ä¿å­˜
    japanese_encoders = {
        'éƒ½é“åºœçœŒ': None,  # Prefecture not needed for Tokyo-only
        'å¸‚åŒºç”ºæ‘': municipality_encoder,
        'åœ°åŒºå': district_encoder
    }
    
    # 2. äº’æ›æ€§ã®ã‚ã‚‹ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ä½œæˆ
    logger.info("Creating scaler...")
    
    # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã«è¿‘ã„ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’ä½œæˆ
    sample_data = np.array([
        [0, 0, 50, 40, 0],     # æœ€å°å€¤ä»˜è¿‘
        [5, 3, 100, 80, 10],   # å…¸å‹çš„
        [10, 6, 150, 120, 20], # å¤§ãã‚
        [11, 8, 200, 150, 30], # æœ€å¤§å€¤ä»˜è¿‘
        [3, 2, 75, 60, 5],     # ä¸­é–“å€¤
    ])
    
    scaler = StandardScaler()
    scaler.fit(sample_data)
    
    # 3. å¤ã„joblibãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨äº’æ›æ€§ã®ã‚ã‚‹å½¢å¼ã§ä¿å­˜
    logger.info("Saving with legacy compatibility...")
    
    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ä¿å­˜
    encoders_path = os.path.join(output_dir, 'label_encoders.joblib')
    joblib.dump(japanese_encoders, encoders_path, compress=3)  # è»½ã„åœ§ç¸®
    logger.info(f"Encoders saved: {encoders_path}")
    
    # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ä¿å­˜
    scaler_path = os.path.join(output_dir, 'scaler.joblib')
    joblib.dump(scaler, scaler_path, compress=3)  # è»½ã„åœ§ç¸®
    logger.info(f"Scaler saved: {scaler_path}")
    
    # 4. æ—¢å­˜ã®æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’ã‚³ãƒ”ãƒ¼ï¼ˆã“ã‚Œã¯å‹•ä½œã™ã‚‹ã“ã¨ãŒç¢ºèªæ¸ˆã¿ï¼‰
    existing_model = os.path.join(output_dir, 'optimized_model.joblib')
    target_model = os.path.join(output_dir, 'valuation_model.joblib')
    
    if os.path.exists(existing_model):
        import shutil
        shutil.copy2(existing_model, target_model)
        logger.info(f"Model copied: {existing_model} -> {target_model}")
    
    # 5. ç‰¹å¾´é‡åˆ—æƒ…å ±
    feature_cols = ['Municipality_encoded', 'DistrictName_encoded', 'åœŸåœ°é¢ç©', 'å»ºç‰©é¢ç©', 'ç¯‰å¹´æ•°']
    feature_path = os.path.join(output_dir, 'feature_columns.joblib')
    joblib.dump(feature_cols, feature_path, compress=3)
    logger.info(f"Feature columns saved: {feature_path}")
    
    # 6. æ¤œè¨¼ãƒ†ã‚¹ãƒˆ
    logger.info("Performing validation test...")
    try:
        # å†èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
        test_encoders = joblib.load(encoders_path)
        test_scaler = joblib.load(scaler_path)
        test_features = joblib.load(feature_path)
        
        logger.info(f"âœ… Encoders loaded: {list(test_encoders.keys())}")
        logger.info(f"âœ… Scaler loaded: {type(test_scaler).__name__}")
        logger.info(f"âœ… Features loaded: {test_features}")
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ†ã‚¹ãƒˆ
        if test_encoders['å¸‚åŒºç”ºæ‘'] is not None:
            encoded = test_encoders['å¸‚åŒºç”ºæ‘'].transform(['æ¸‹è°·åŒº'])
            logger.info(f"âœ… Municipality encoding test: æ¸‹è°·åŒº -> {encoded[0]}")
            
        if test_encoders['åœ°åŒºå'] is not None:
            encoded = test_encoders['åœ°åŒºå'].transform([''])
            logger.info(f"âœ… District encoding test: '' -> {encoded[0]}")
        
        # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ãƒ†ã‚¹ãƒˆ
        dummy_data = np.array([[5, 2, 100, 80, 10]])
        scaled = test_scaler.transform(dummy_data)
        logger.info(f"âœ… Scaler test: {dummy_data.flatten()} -> {scaled.flatten()}")
        
        logger.info("ğŸ‰ All validation tests passed!")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Validation failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def create_info_file():
    """äº’æ›ãƒ¢ãƒ‡ãƒ«æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"""
    output_dir = '/Users/kawabuchieirin/Desktop/satei_app/model-creation/models'
    
    info = {
        "model_type": "Docker Compatible Random Forest",
        "created_for": "NumPy 1.24.3, scikit-learn 1.3.2",
        "features": 5,
        "encoders": {
            "municipalities": ["æ¸‹è°·åŒº", "æ–°å®¿åŒº", "æ¸¯åŒº", "ä¸­å¤®åŒº", "åƒä»£ç”°åŒº", "å“å·åŒº", "ç›®é»’åŒº", "ä¸–ç”°è°·åŒº", "å¤§ç”°åŒº", "æ±Ÿæ±åŒº", "æ–‡äº¬åŒº", "å°æ±åŒº"],
            "districts": ["", "1ä¸ç›®", "2ä¸ç›®", "3ä¸ç›®", "4ä¸ç›®", "5ä¸ç›®", "è¥¿", "æ±", "å—", "åŒ—"]
        },
        "notes": "Created to resolve numpy._core compatibility issues in Docker environment"
    }
    
    info_path = os.path.join(output_dir, 'docker_compatible_info.json')
    with open(info_path, 'w', encoding='utf-8') as f:
        json.dump(info, f, indent=2, ensure_ascii=False)
    
    logger.info(f"Info file created: {info_path}")

if __name__ == "__main__":
    success = create_minimal_compatible_files()
    if success:
        create_info_file()
        logger.info("âœ… Docker-compatible model files creation completed!")
    else:
        logger.error("âŒ Failed to create compatible files")
        exit(1)